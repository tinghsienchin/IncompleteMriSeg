import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.autograd import Variable
import numpy as np

smooth = 0.00001

class BinaryDiceLoss(nn.Module):
	"""docstring for BinaryDiceLoss"""
	def __init__(self, smooth = 1, p = 2, reduction='mean'):
		super(BinaryDiceLoss, self).__init__()
		self.smooth = smooth
		self.p = p
		self.reduction = reduction

	def forward(self, predict, target):
		assert predict.shape[0] == target.shape[0], "predict & target batch size don't match"
		predict = predict.contiguous().view(predict.shape[0], -1)
		target = target.contiguous().view(predict.shape[0], -1)

		num = torch.sum(torch.mul(predict, target), dim=1)
		den = torch.sum(predict, dim=1) + torch.sum(target, dim=1) + self.smooth

		dice_score = 2*num / den
		dice_loss = 1 - dice_score

		dice_loss_avg = dice_loss[target[:,0]!=-1].sum()

		return dice_loss_avg

class DiceLoss4MOTS(nn.Module):
	"""docstring for DiceLoss4MOTS"""
	def __init__(self, weight = None, ignore_index = None, num_classes=3, **kwargs):
		super(DiceLoss4MOTS, self).__init__()
		self.kwargs = kwargs
		self.weight = weight
		self.ignore_index = ignore_index
		self.num_classes = num_classes
		self.dice = BinaryDiceLoss(**self.kwargs)

	def forward(self, predict, target):

		total_loss = []
		predict = F.sigmoid(predict)

		for i in range(self.num_classes):
			if i !=self.ignore_index:
				dice_loss = self.dice(predict[:, i], target[:, i])
				if self.weight is not None:
					assert self.weight.shape[0] == self.num_classes, \
					       'Expect weight shape [{}], get[{}]'.format(self.num_classes, self.weight.shape[0])
					dice_loss *= self.weights[i]
				total_loss.append(dice_loss)

		total_loss = torch.stack(total_loss)
		total_loss = total_loss[total_loss==total_loss]

		return total_loss.sum()/total_loss.shape[0]

class CELoss4MOTS(nn.Module):
	"""docstring for CELoss4MOTS"""
	def __init__(self, ignore_index=None, num_classes=3, **kwargs):
		super(CELoss4MOTS, self).__init__()
		self.kwargs = kwargs
		self.num_classes = num_classes
		self.ignore_index = ignore_index
		self.criterion = nn.BCEWithLogitsLoss(reduction='none')

	def weight_function(self, mask):
		weights = torch.ones_like(mask).float()
		voxels_sum = mask.shape[0] * mask.shape[1] * mask.shape[2]
		for i in range(2):
			voxels_i = [mask == i][0].sum().cpu().numpy()
			w_i = np.log(voxels_sum / voxels_i).astype(np.float32)
			weights = torch.where(mask == i, w_i * torch.ones_like(weights).float(), weights)

		return weights

	def forward(self, predict, target):
		assert predict.shape == target.shape, 'predict & target shape do not match'

		total_loss = []
		for i in range(self.num_classes):
			if i != self.ignore_index:
				ce_loss = self.criterion(predict[:,i], target[:,i])
				ce_loss = torch.mean(ce_loss, dim = [1,2,3])

				ce_loss_avg = ce_loss[target[:, i, 0, 0, 0] != -1].sum() / ce_loss[target[:, i, 0, 0, 0] != -1].shape[0]

				total_loss.append(ce_loss_avg)

		total_loss = torch.stack(total_loss)
		total_loss = total_loss[total_loss == total_loss]

		return total_loss.sum()/total_loss.shape[0]

def dice(logits, targets, class_index):
	probs = F.sigmoid(logits[:, class_index, :, :, :])
	inter = torch.sum(probs*targets[:, class_index, :, :, :])
	union = torch.sum(probs) +torch.sum(targets[:, class_index, :, :, :])
	dice = (2.*inter + smooth) / (union + smooth)
	return 1-dice